{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is the continuation of the Reddit-Scraping.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from reddit.csv - refer to Scraping-Part-1.ipynb\n",
    "final_df = pd.read_csv('./reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4319, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knock-knock</td>\n",
       "      <td>Who's there?   \\n**A parrot!**  \\nA parrot who...</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>1552069053</td>\n",
       "      <td>motsanciens</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How did the dentist suddenly become a brain su...</td>\n",
       "      <td>A slip of the hand.</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>1552069079</td>\n",
       "      <td>roastedtoperfection</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hate build a bear. I took my chihauhua there...</td>\n",
       "      <td>AND the stuffed animal they gave me keeps bark...</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>1552069382</td>\n",
       "      <td>RikorperationYT</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An English Teacher And The Pope Was Sitting Ne...</td>\n",
       "      <td>He was reading a challenging book, and was ver...</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>1552069428</td>\n",
       "      <td>GangstaKev</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I looked left, then I looked right. I looked l...</td>\n",
       "      <td>Then I pulled out... she wasn’t pleased.</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>1552069459</td>\n",
       "      <td>Windwaker85</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                        Knock-knock   \n",
       "1  How did the dentist suddenly become a brain su...   \n",
       "2  I hate build a bear. I took my chihauhua there...   \n",
       "3  An English Teacher And The Pope Was Sitting Ne...   \n",
       "4  I looked left, then I looked right. I looked l...   \n",
       "\n",
       "                                            selftext subreddit  created_utc  \\\n",
       "0  Who's there?   \\n**A parrot!**  \\nA parrot who...     Jokes   1552069053   \n",
       "1                                A slip of the hand.     Jokes   1552069079   \n",
       "2  AND the stuffed animal they gave me keeps bark...     Jokes   1552069382   \n",
       "3  He was reading a challenging book, and was ver...     Jokes   1552069428   \n",
       "4          Then I pulled out... she wasn’t pleased.      Jokes   1552069459   \n",
       "\n",
       "                author  num_comments  score  is_self   timestamp  \n",
       "0          motsanciens             0      0     True  2019-03-08  \n",
       "1  roastedtoperfection             0      4     True  2019-03-08  \n",
       "2      RikorperationYT             0      2     True  2019-03-08  \n",
       "3           GangstaKev             2      3     True  2019-03-08  \n",
       "4          Windwaker85             0      2     True  2019-03-08  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first 5 rows\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           object\n",
       "selftext        object\n",
       "subreddit       object\n",
       "created_utc      int64\n",
       "author          object\n",
       "num_comments     int64\n",
       "score            int64\n",
       "is_self           bool\n",
       "timestamp       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             0\n",
       "selftext        118\n",
       "subreddit         0\n",
       "created_utc       0\n",
       "author            0\n",
       "num_comments      0\n",
       "score             0\n",
       "is_self           0\n",
       "timestamp         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for nulls in final_df\n",
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nulls with inplace true\n",
    "final_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for nulls after dropna\n",
    "final_df['selftext'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[removed]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     667\n",
       "[deleted]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      11\n",
       "I don’t have 2020 vision\\n\\nThis is the only day you can upvote this\\n\\nEDIT: Thank you sm for r/all ! Happy New Years!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         4\n",
       "Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\\n\\n* Learning resources (e.g. books, tutorials, videos)  \\n* Traditional education (e.g. schools, degrees, electives)  \\n* Alternative education (e.g. online courses, bootcamps)  \\n* Job search questions (e.g. resumes, applying, career prospects)  \\n* Elementary questions (e.g. where to start, what next)  \\n\\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](https://www.reddit.com/r/datascience/wiki/resources) pages on our wiki.  \\n\\n[You can also search for past weekly threads here](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;t=month).\\n\\n^(Last configured: 2019-02-17 09:32 AM EDT)\\n      3\n",
       "That's the only part of their body that is washed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              3\n",
       "Name: selftext, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the values in selftext using value counts\n",
    "final_df['selftext'].value_counts(ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new df 'final_df_2' exclude '[removed]' and '[deleted]'\n",
    "final_df_2 = final_df[(final_df.selftext != '[removed]') & (final_df.selftext != '[deleted]')].copy(deep = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I don’t have 2020 vision\\n\\nThis is the only day you can upvote this\\n\\nEDIT: Thank you sm for r/all ! Happy New Years!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    4\n",
       "That's the only part of their body that is washed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         3\n",
       "Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\\n\\n* Learning resources (e.g. books, tutorials, videos)  \\n* Traditional education (e.g. schools, degrees, electives)  \\n* Alternative education (e.g. online courses, bootcamps)  \\n* Job search questions (e.g. resumes, applying, career prospects)  \\n* Elementary questions (e.g. where to start, what next)  \\n\\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](https://www.reddit.com/r/datascience/wiki/resources) pages on our wiki.  \\n\\n[You can also search for past weekly threads here](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;t=month).\\n\\n^(Last configured: 2019-02-17 09:32 AM EDT)\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 3\n",
       "\"Because he used to live in a brothel\" says the shopkeeper. She pays $15.\\n\\nWhen she gets home the parrot says: \"Fuck me, a new brothel!\" The woman laughs.\\n\\nWhen her daughters get home the parrot says: \"Fuck me, 2 new prozzies!\" The girls laughs too.\\n\\nWhen the dad gets home the parrot says: \"Fuck me Pete, haven't seen you for weeks!\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       3\n",
       "I would appreciate hearing some opinions from those in the know about my future study and career plans.\\n\\n**Background:**\\n\\nI have a masters in pure mathematics from a highly ranked UK university. I did almost no programming at university and a little statistics. The focus of my masters was topology, manifolds, Lie groups etc, if that means anything to anyone here :) Of course I have studied Linear Algebra in depth and am really enjoying learning the statistics that is new to me. Since graduating I have been teaching mathematics at High School level in International Schools around the globe for the past 6 years. But now I am looking to try something new.\\n\\n**Study so far:**\\n\\nFor the past 2 years I have been studying Computer Science topics and programming. I have taken Database Management Systems (MySQL) and Intermediate Software Design in Python through Foothill College online. Then I have done various MOOCs including CS50 and Stanford Algorithms. I am almost finished with \"Python For Data Science And Machine Learning\" on udemy and have really enjoyed it. It doesn't go into the mathematics, so at the same time I've been reading ISLR. I'm then going to go through ISLR once again but doing the exercises in R. I'm looking forward to then moving onto The Elements of Statistical Learning to really get into the details of the mathematics. I'm also picking up a lot more books when I'm home for winter break that I'm going to sink myself into :D\\n\\nI have come to the conclusion that a career in Data Science is going to combine my love of mathematics, data, programming etc into one I will really enjoy and (hopefully) be good at.\\n\\n**Personal situation:**\\n\\nI will be working full time (studying in spare time) until June 2019. Then I am in the lucky position of being able to stay at home, whilst my (very supportive) partner works another year until June 2020. I will be both studying and being a parent (with childcare). I live in a developing country with few options for local experience in the field (though I need to research this further). The plan is then to move to my partners home city (Seattle) in the summer of 2020 and for me to find a job :)\\n\\n**Options for the next year and a half:**\\n\\nSo I am currently weighing up my options for the next year and a half before we move around the globe. I have looked at some data science bootcamps and I could do similar stuff on my own: building up a portfolio, writing a blog, contributing to open source software, entering kaggle comps, building personal projects etc. The great thing is I will be able to go much more in-depth with the topics compared to a bootcamp as I'll have so much more time. The downside is I will not have the career guidance, interview prep, help with LinkedIn etc.\\n\\nAnother option I am looking at is starting the Georgia Tech OMS Analytics Masters. I do like the structured learning of a full university course and the course descriptions [sound really interesting](https://pe.gatech.edu/degrees/analytics?section=curriculum). It does sound like it would be a real challenge to finish in a year and I think I would only be able to start Fall 2019.\\n\\nSorry for the wall of text. I'd appreciate any feedback, ideas, words of encouragement, thoughts on my situation.\\n\\n**TLDR: Mathematics background transitioning to data science. Has a year and a half (with one year of full-time study, portfolio work, maybe masters) until move to the US and trying to change to a career in Data Science. Advice?**    2\n",
       "Name: selftext, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the values in selftext using value counts after deleting above\n",
    "final_df_2['selftext'].value_counts(ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knock-knock</td>\n",
       "      <td>Who's there?   \\n**A parrot!**  \\nA parrot who...</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>1552069053</td>\n",
       "      <td>motsanciens</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How did the dentist suddenly become a brain su...</td>\n",
       "      <td>A slip of the hand.</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>1552069079</td>\n",
       "      <td>roastedtoperfection</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hate build a bear. I took my chihauhua there...</td>\n",
       "      <td>AND the stuffed animal they gave me keeps bark...</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>1552069382</td>\n",
       "      <td>RikorperationYT</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An English Teacher And The Pope Was Sitting Ne...</td>\n",
       "      <td>He was reading a challenging book, and was ver...</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>1552069428</td>\n",
       "      <td>GangstaKev</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I looked left, then I looked right. I looked l...</td>\n",
       "      <td>Then I pulled out... she wasn’t pleased.</td>\n",
       "      <td>Jokes</td>\n",
       "      <td>1552069459</td>\n",
       "      <td>Windwaker85</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-03-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                        Knock-knock   \n",
       "1  How did the dentist suddenly become a brain su...   \n",
       "2  I hate build a bear. I took my chihauhua there...   \n",
       "3  An English Teacher And The Pope Was Sitting Ne...   \n",
       "4  I looked left, then I looked right. I looked l...   \n",
       "\n",
       "                                            selftext subreddit  created_utc  \\\n",
       "0  Who's there?   \\n**A parrot!**  \\nA parrot who...     Jokes   1552069053   \n",
       "1                                A slip of the hand.     Jokes   1552069079   \n",
       "2  AND the stuffed animal they gave me keeps bark...     Jokes   1552069382   \n",
       "3  He was reading a challenging book, and was ver...     Jokes   1552069428   \n",
       "4          Then I pulled out... she wasn’t pleased.      Jokes   1552069459   \n",
       "\n",
       "                author  num_comments  score  is_self   timestamp  \n",
       "0          motsanciens             0      0     True  2019-03-08  \n",
       "1  roastedtoperfection             0      4     True  2019-03-08  \n",
       "2      RikorperationYT             0      2     True  2019-03-08  \n",
       "3           GangstaKev             2      3     True  2019-03-08  \n",
       "4          Windwaker85             0      2     True  2019-03-08  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first 5 rows\n",
    "final_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Jokes\n",
       "1    Jokes\n",
       "2    Jokes\n",
       "3    Jokes\n",
       "4    Jokes\n",
       "Name: subreddit, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_2['subreddit'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jokes          2129\n",
       "datascience    1394\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value counts for subreddit\n",
    "final_df_2['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicates\n",
    "final_df_2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column 'is_datascience', if a subreddit is related to datascience, keep 1, otherwise 0\n",
    "final_df_2['is_datascience'] = final_df_2['subreddit'].map({'Jokes':0, 'datascience':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       0\n",
       "3100    1\n",
       "2115    0\n",
       "2907    1\n",
       "3976    1\n",
       "1059    0\n",
       "2988    1\n",
       "1426    0\n",
       "3930    1\n",
       "288     0\n",
       "Name: is_datascience, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_2['is_datascience'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Knock-knock Who's there?   \\n**A parrot!**  \\n...\n",
       "1       How did the dentist suddenly become a brain su...\n",
       "2       I hate build a bear. I took my chihauhua there...\n",
       "3       An English Teacher And The Pope Was Sitting Ne...\n",
       "4       I looked left, then I looked right. I looked l...\n",
       "5       A man goes the cinema to see the first Harry P...\n",
       "6       Hey, want a book full of jokes? Here's a copy ...\n",
       "7       I just heard that the first manned mission of ...\n",
       "8       I’m 6 foot 3 inches Those are two different me...\n",
       "9       I was going to make a sexual harassment joke ....\n",
       "10      How many vegans does it take to eat a cheese a...\n",
       "11      The difference between being hungry or horny i...\n",
       "12                I'm not racist, BUT... ...I like trains\n",
       "13      Did you hear how loud that hooker's fart was? ...\n",
       "14      What’s the difference between a hippo and a zi...\n",
       "15      I'm not sure how I feel about this rash on my ...\n",
       "16      Why did the lottery winner want to stay homele...\n",
       "17      Why are women afraid of psychological treatmen...\n",
       "18      What do you call a racist cake? A cake-cake-cake.\n",
       "19      The ElePhantom of myOpia This is a joke for an...\n",
       "20      What is the difference between a pirate and R....\n",
       "22      A fathers son asked him to play a game togethe...\n",
       "23      What do you call an educated woman in a third ...\n",
       "24      “Sorry about the temperature down the mine tod...\n",
       "25      Cowboy: I've had a bellyful of you... Draw! Me...\n",
       "27      What tells on you all the time, and makes your...\n",
       "28      A church boy walks into a bar It's his priests...\n",
       "29      I was the runner up in the 2012 world boner ch...\n",
       "32      Lasik Eye Surgery A few years ago, I finally d...\n",
       "33      I can do a Snake impression My friends say it'...\n",
       "                              ...                        \n",
       "4282    Retail Data Science Hi All,\\n\\nI have recently...\n",
       "4283    For those of you working on research, how do y...\n",
       "4284    Why is bootstrapping not effective in finding ...\n",
       "4285    My First Analysis Hi,\\n\\nI'm a college student...\n",
       "4286    Weekly 'Entering &amp; Transitioning' Thread. ...\n",
       "4287    Distinguishing data pre-processing vs. mining ...\n",
       "4288    People who work as Data Analyst, what are your...\n",
       "4289    Weird Glitch With Ipod touch notes Hey guys, I...\n",
       "4291    Want to start career as data scientist profess...\n",
       "4292    Script Optimizations? Hello r/datascience,\\n\\n...\n",
       "4294    Getting Started in DS: Am I doing it right? He...\n",
       "4295    Beating the state-of-the-art in NLP with HMTL ...\n",
       "4297    Scrapping personal FB page question So I've tr...\n",
       "4298    Data Analyst / Science Job Applications Have y...\n",
       "4299    I had an odd experience today at a job intervi...\n",
       "4300    New to DS. Need best Imputation Method to acco...\n",
       "4301    new kaggle is out,if I try and get 19000th spo...\n",
       "4302    Data Enthusiasts Paid Interview (Graduate Rese...\n",
       "4303    BS in Econ &gt; Investment Banking &gt; Data S...\n",
       "4304    What do you think when you hear \"hacker statis...\n",
       "4305    Electrical Engineer to Data Scientist I’ve wor...\n",
       "4306    Marketing Mix Model Help  \\n\\nI am taking over...\n",
       "4307    Best project/s I can do to improve chances of ...\n",
       "4308    Object Detection (with Bounding Box) in Pytorc...\n",
       "4310    Just read this interview with Roger Peng about...\n",
       "4311    Database structure at your company How does yo...\n",
       "4312    Global Text Analytics Market – Size, Outlook, ...\n",
       "4313    Bored data analyst wants to get into data scie...\n",
       "4316    Looking for advice about which data science cu...\n",
       "4317    Statistics question on data cleaning Your data...\n",
       "Name: title_text, Length: 3523, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column 'title_text' by merging title and selftext\n",
    "final_df_2['title_text'] = final_df_2['title'] + ' ' + final_df_2['selftext']\n",
    "final_df_2['title_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the cleaned data to a csv - reddit_EDA.csv\n",
    "final_df_2.to_csv('./reddit_EDA.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
